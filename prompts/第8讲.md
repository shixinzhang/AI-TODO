
当前实现的问题
手动解析 SSE 格式，容易出错
缓冲区处理复杂，需要手动分割 \n\n
错误处理不够完善
定时器逻辑复杂，容易导致重复


使用框架简化开发



**现代化框架**：Vercel AI SDK 把这些最佳实践都内置了，让你用 50 行代码就能实现手动封装需要 200 行的功能。


手动封装让你理解了 API 调用的底层原理，但在实际项目中，我们通常会使用更现代化的框架来简化开发。**Vercel AI SDK** 就是这样一个全栈神器，它把我们刚才手动实现的功能都内置了，而且做得更好。



6.1 为什么选择 Vercel AI SDK？



如果你是全栈开发者，特别是使用 Next.js 的开发者，Vercel AI SDK 是 2025 年的最佳选择。它的核心价值在于：



6.1.1 Unified API：抹平所有厂商差异**



还记得我们在开篇强调的 OpenAI 兼容协议吗？Vercel AI SDK 更进一步，它提供了统一的抽象层，让你可以用同样的代码调用 OpenAI、Anthropic、Google、DeepSeek、通义千问等所有主流模型。



```javascript

import { createOpenAI } from '@ai-sdk/openai';

import { generateText } from 'ai';



// 调用 OpenAI

const openai = createOpenAI({ apiKey: process.env.OPENAI_API_KEY });

const result1 = await generateText({

model: openai('gpt-4'),

prompt: '你好'

});



// 调用 DeepSeek（通过 OpenAI 兼容接口）

const deepseek = createOpenAI({

apiKey: process.env.DEEPSEEK_API_KEY,

baseURL: 'https://api.deepseek.com'

});

const result2 = await generateText({

model: deepseek('deepseek-chat'),

prompt: '你好'

});

```



6.1.2 useChat 钩子：完美处理前后端流式数据传输**



我们刚才手动实现了流式响应，需要写很多代码来处理 SSE、ReadableStream、状态管理。Vercel AI SDK 的 `useChat` 钩子把这一切都封装好了：



```javascript

// app/api/chat/route.js（后端）

import { createOpenAI } from '@ai-sdk/openai';

import { streamText } from 'ai';



const deepseek = createOpenAI({

apiKey: process.env.DEEPSEEK_API_KEY,

baseURL: 'https://api.deepseek.com'

});



export async function POST(req) {

const { messages } = await req.json();



const result = await streamText({

model: deepseek('deepseek-chat'),

messages

});



return result.toDataStreamResponse(); // 自动处理流式响应

}

```



```javascript

// components/Chat.jsx（前端）

'use client';

import { useChat } from 'ai/react';



export default function Chat() {

const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat({

api: '/api/chat'

});



return (









{messages.map(m => (





{m.role}: {m.content}





))}

{isLoading &&

AI 正在思考...

}















value={input}

onChange={handleInputChange}

placeholder="输入消息..."

/>





发送











);

}

```



看到了吗？前端只需要一个 `useChat` 钩子，就自动处理了：

- ✅ 流式数据接收

- ✅ 消息状态管理

- ✅ 加载状态显示

- ✅ 表单提交处理

- ✅ 错误处理



6.1.3 内置重试、超时、错误处理



我们手动实现的重试机制、超时控制、错误处理，Vercel AI SDK 都内置了：



```javascript

import { createOpenAI } from '@ai-sdk/openai';

import { generateText } from 'ai';



const deepseek = createOpenAI({

apiKey: process.env.DEEPSEEK_API_KEY,

baseURL: 'https://api.deepseek.com',

fetch: (url, options) => {

// 自定义 fetch，可以添加重试逻辑

return fetch(url, {

...options,

signal: AbortSignal.timeout(120000) // 120 秒超时

});

}

});



try {

const result = await generateText({

model: deepseek('deepseek-chat'),

prompt: '你好',

maxRetries: 3 // 内置重试

});

console.log(result.text);

} catch (error) {

console.error('调用失败：', error);

}

```



6.2 完整示例：用 Vercel AI SDK 构建聊天应用



让我们用 Vercel AI SDK 重写之前的聊天应用，看看代码能简化多少：



```javascript

// app/api/chat/route.js

import { createOpenAI } from '@ai-sdk/openai';

import { streamText } from 'ai';



const deepseek = createOpenAI({

apiKey: process.env.DEEPSEEK_API_KEY,

baseURL: 'https://api.deepseek.com'

});



export async function POST(req) {

const { messages } = await req.json();



const result = await streamText({

model: deepseek('deepseek-chat'),

messages,

temperature: 0.7,

maxTokens: 4096,

onFinish: ({ text, usage }) => {

// 记录 token 消耗

console.log(`生成完成，消耗 ${usage.totalTokens} tokens`);

}

});



return result.toDataStreamResponse();

}

```



```javascript

// app/page.jsx

'use client';

import { useChat } from 'ai/react';



export default function ChatPage() {

const {

messages,

input,

handleInputChange,

handleSubmit,

isLoading,

error

} = useChat({

api: '/api/chat',

onError: (error) => {

console.error('聊天错误：', error);

}

});



return (





{/* 消息列表 */}





{messages.map(m => (



key={m.id}

className={`p-4 rounded-lg ${

m.role === 'user' ? 'bg-blue-100 ml-auto' : 'bg-gray-100'

} max-w-[80%]`}

>





{m.role === 'user' ? '你' : 'AI'}







{m.content}







))}



{isLoading && (







AI





正在思考...







)}



{error && (





错误：{error.message}





)}







{/* 输入框 */}







value={input}

onChange={handleInputChange}

placeholder="输入消息..."

className="flex-1 p-2 border rounded-lg"

disabled={isLoading}

/>



type="submit"

disabled={isLoading}

className="px-4 py-2 bg-blue-500 text-white rounded-lg disabled:bg-gray-300"

>

发送









);

}

```



6.3 核心优势总结



| 功能 | 手动封装 | Vercel AI SDK |

|------|---------|--------------|

| 流式响应 | 需要手动处理 SSE、ReadableStream | `useChat` 钩子自动处理 |

| 状态管理 | 需要手动管理消息、加载状态 | 内置状态管理 |

| 重试机制 | 需要集成 p-retry | 内置 `maxRetries` |

| 超时控制 | 需要手动设置 | 支持自定义 fetch |

| 多厂商支持 | 需要手动适配 | Unified API 统一接口 |

| 错误处理 | 需要手动 try-catch | 内置 `onError` 回调 |

| Token 统计 | 需要手动计算 | 内置 `usage` 统计 |

| 代码量 | 200+ 行 | 50 行 |



### 何时使用 Vercel AI SDK？



**适合场景**：

- ✅ Next.js 全栈应用

- ✅ 需要快速构建聊天界面

- ✅ 需要支持多个模型厂商

- ✅ 需要流式响应和良好的用户体验



**不适合场景**：

- ❌ 纯后端服务（没有前端界面）

- ❌ 非 React 技术栈

- ❌ 需要极致性能优化的场景



如果你的项目符合适合场景，强烈建议使用 Vercel AI SDK。它不仅能节省大量开发时间，还能提供更好的用户体验和更稳定的系统。