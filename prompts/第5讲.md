
```
import OpenAI from 'openai';

const client = new OpenAI({ apiKey: "sk-xxx" });

const response = await client.chat.completions.create({ model: "gpt-4", messages: [{ role: "user", content: "你好" }] });
```

```
import OpenAI from 'openai';



// 调用 DeepSeek

const client = new OpenAI({

apiKey: "your-deepseek-key",

baseURL: "https://api.deepseek.com"

});



// 调用阿里通义千问

const client = new OpenAI({

apiKey: "your-qwen-key",

baseURL: "https://dashscope.aliyuncs.com/compatible-mode/v1"

});



// 调用字节豆包

const client = new OpenAI({

apiKey: "your-doubao-key",

baseURL: "https://ark.cn-beijing.volces.com/api/v3"

});
```

```javascript

import OpenAI from 'openai';



const client = new OpenAI({

apiKey: process.env.DEEPSEEK_API_KEY,

baseURL: "https://api.deepseek.com"

});



// 场景 1：代码生成、数学计算、JSON 提取 → Temperature = 0

const response1 = await client.chat.completions.create({

model: "deepseek-chat",

messages: [{ role: "user", content: "用 JavaScript 写一个快速排序" }],

temperature: 0 // 绝对理性，确保输出稳定

});



// 场景 2：文案创作、角色扮演、头脑风暴 → Temperature = 0.7-0.9

const response2 = await client.chat.completions.create({

model: "deepseek-chat",

messages: [{ role: "user", content: "写一段科幻小说的开头" }],

temperature: 0.8 // 高创意，输出更有想象力

});

```


```javascript

// ✅ 推荐：只调 Temperature

const response = await client.chat.completions.create({

model: "deepseek-chat",

messages: [{ role: "user", content: "你好" }],

temperature: 0.7,

top_p: 1.0 // 保持默认

});



// ❌ 不推荐：同时调整两个参数

const response = await client.chat.completions.create({

model: "deepseek-chat",

messages: [{ role: "user", content: "你好" }],

temperature: 0.7,

top_p: 0.9 // 逻辑会混乱

});

```

```javascript

// 场景 1：简短回答（如客服机器人）

const response1 = await client.chat.completions.create({

model: "deepseek-chat",

messages: [{ role: "user", content: "什么是 AI？" }],

max_tokens: 100 // 限制在 100 个 token 以内

});



// 场景 2：长文本生成（如文章创作）

const response2 = await client.chat.completions.create({

model: "deepseek-chat",

messages: [{ role: "user", content: "写一篇关于 AI 的文章" }],

max_tokens: 4096 // 允许更长的输出

});

```

```javascript

// 调试阶段：固定 Seed，确保输出可复现

const response1 = await client.chat.completions.create({

model: "deepseek-chat",

messages: [{ role: "user", content: "写一个产品介绍" }],

temperature: 0.7,

seed: 12345 // 固定随机种子

});



// 第二次调用，输出结果会完全一样

const response2 = await client.chat.completions.create({

model: "deepseek-chat",

messages: [{ role: "user", content: "写一个产品介绍" }],

temperature: 0.7,

seed: 12345 // 相同的 Seed，相同的输出

});

```


```javascript

// 通过硅基流动调用 DeepSeek（更稳定）

const client = new OpenAI({

apiKey: process.env.SILICONFLOW_API_KEY,

baseURL: "https://api.siliconflow.cn/v1"

});



const response = await client.chat.completions.create({

model: "deepseek-ai/DeepSeek-V3",

messages: [{ role: "user", content: "你好" }]

});

```

```javascript

// 硅基流动的免费模型调用

const client = new OpenAI({

apiKey: process.env.SILICONFLOW_API_KEY,

baseURL: "https://api.siliconflow.cn/v1"

});



const response = await client.chat.completions.create({

model: "Qwen/Qwen2.5-7B-Instruct", // 完全免费

messages: [{ role: "user", content: "你好" }]

});

```

```javascript

// 调用 Qwen-Long 处理超长文档

const client = new OpenAI({

apiKey: process.env.QWEN_API_KEY,

baseURL: "https://dashscope.aliyuncs.com/compatible-mode/v1"

});



const response = await client.chat.completions.create({

model: "qwen-long",

messages: [{ role: "user", content: "请分析这份 100 页的合同..." }],
max_tokens=4096,  # 限制输出长度

});

```

```javascript

// 调用火山方舟豆包模型

const client = new OpenAI({

apiKey: process.env.DOUBAO_API_KEY,

baseURL: "https://ark.cn-beijing.volces.com/api/v3"

});



const response = await client.chat.completions.create({

model: "doubao-seed-1.6-flash",

messages: [{ role: "user", content: "你好" }]

});

```

## 4.5 

```javascript

// 调用腾讯混元（免费模型）

const client = new OpenAI({

apiKey: process.env.HUNYUAN_API_KEY,

baseURL: "https://api.hunyuan.cloud.tencent.com/v1"

});



const response = await client.chat.completions.create({

model: "hunyuan-lite", // 完全免费

messages: [{ role: "user", content: "你好" }]

});

```

# 5

```javascript

import pRetry from 'p-retry';

import OpenAI from 'openai';



const client = new OpenAI({

	apiKey: process.env.DEEPSEEK_API_KEY,
	
	baseURL: "https://api.deepseek.com",
	
	timeout: 120000 // 超时设置：120 秒
	
	});



async function callLLM(prompt) {

	return pRetry(
	
		async () => {
		
			const response = await client.chat.completions.create({
			
			model: "deepseek-chat",
			
			messages: [{ role: "user", content: prompt }]
			
			});
			
			return response.choices[0].message.content;
		
		},
	
		{
		
			retries: 3, // 最多重试 3 次
			
			factor: 2, // 指数因子
			
			minTimeout: 1000, // 最小延迟 1 秒
			
			maxTimeout: 10000, // 最大延迟 10 秒
			
			onFailedAttempt: (error) => {
			
			console.log(`尝试 ${error.attemptNumber} 失败，还剩 ${error.retriesLeft} 次重试`);
			
			}
		
		}

);

}



// 调用时自动重试

try {

	const result = await callLLM("写一段代码");
	
	console.log(result);

} catch (error) {

	console.error("重试 3 次后仍然失败：", error);

}

```


# SSE

```
// 开始消息
data: {"type":"start","message":"开始生成..."}\n\n

// 数据块消息
data: {"type":"chunk","content":"Hello"}\n\n
data: {"type":"chunk","content":" World"}\n\n

// 结束消息
data: {"type":"done","message":"生成完成"}\n\n
```



步骤 1：设置 SSE 响应头

```
res.setHeader('Content-Type', 'text/event-stream')  // SSE 标准格式
res.setHeader('Cache-Control', 'no-cache')           // 禁用缓存
res.setHeader('Connection', 'keep-alive')            // 保持连接
res.setHeader('X-Accel-Buffering', 'no')            // 禁用 Nginx 缓冲
```

步骤 2：调用大模型 API（流式）

```
const openai = new OpenAI({
  apiKey: DEEPSEEK_API_KEY,
  baseURL: DEEPSEEK_API_BASE_URL,  // SiliconFlow 的 API 地址
})

const stream = await openai.chat.completions.create({
  model: 'deepseek-ai/DeepSeek-V3.2-Exp',
  messages: [{ role: 'user', content: prompt }],
  temperature: 0.7,
  max_tokens: 2000,
  stream: true,  // 关键：启用流式输出
})
```

步骤 3：发送 SSE 消息

```
// 1. 发送开始消息
res.write(`data: ${JSON.stringify({ type: 'start', message: '开始生成...' })}\n\n`)

// 2. 循环处理流式响应
for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content || ''
  if (content) {
    // SSE 格式：data: {...}\n\n
    res.write(`data: ${JSON.stringify({ type: 'chunk', content })}\n\n`)
  }
}

// 3. 发送结束消息
res.write(`data: ${JSON.stringify({ type: 'done', message: '生成完成' })}\n\n`)
res.end()
```

标准 SSE 格式：data: <JSON>\n\n

- data: 前缀
- JSON 字符串
- 两个换行符 \n\n 作为消息分隔符

消息类型：

- start: 开始生成
- chunk: 内容块
- done: 完成
- error: 错误


前端使用 Fetch API + ReadableStream 手动解析 SSE：

```
// 1. 使用 fetch 发送 POST 请求
const response = await fetch('/api/samples/stream', {
  method: 'POST',
  body: JSON.stringify({ prompt, model })
})

// 2. 获取 ReadableStream
const reader = response.body.getReader()
const decoder = new TextDecoder()
let buffer = ''

// 3. 手动读取流数据
while (true) {
  const { done, value } = await reader.read()
  if (done) break
  
  // 4. 手动解码和解析 SSE 格式
  buffer += decoder.decode(value, { stream: true })
  const lines = buffer.split('\n\n')  // SSE 消息以 \n\n 分隔
  buffer = lines.pop() || ''
  
  // 5. 手动解析 data: {...} 格式
  for (const line of lines) {
    if (line.startsWith('data: ')) {
      const data = JSON.parse(line.slice(6))
      // 处理数据...
    }
  }
}
```

EventSource 的限制
EventSource 更适合 SSE，但有一个限制：
EventSource 只支持 GET 请求，不支持 POST
当前需要 POST 传递 prompt 和 model 参数

```
    const es = new EventSource('/api/feed');
    
    es.onopen = () => {
      setIsConnected(true);
    };
    
    es.onmessage = (event) => {
      const data = JSON.parse(event.data);
      setMessages(prev => [...prev, data]);
    };
    
    es.onerror = () => {
      setIsConnected(false);
    };
```


当前实现的问题
手动解析 SSE 格式，容易出错
缓冲区处理复杂，需要手动分割 \n\n
错误处理不够完善
定时器逻辑复杂，容易导致重复


```
# Supabase 配置
# 从 Supabase Dashboard -> Settings -> API 获取这些值
NEXT_PUBLIC_SUPABASE_URL=your_supabase_project_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key

# API 配置（可选）
NEXT_PUBLIC_API_BASE_URL=https://api.example.com
NEXT_PUBLIC_APP_ID=your_app_id

# DeepSeek API 配置
# 从 https://platform.deepseek.com 获取 API Key
# 用于 AI 任务拆解功能，如果不配置，拆解功能将无法使用
DEEPSEEK_API_KEY=your_deepseek_api_key

```


```javascript

// Next.js 内置支持，无需额外配置

const client = new OpenAI({

apiKey: process.env.DEEPSEEK_API_KEY,

baseURL: process.env.DEEPSEEK_BASE_URL

});



// Node.js 项目需要安装 dotenv

import dotenv from 'dotenv';

dotenv.config();



const client = new OpenAI({

apiKey: process.env.DEEPSEEK_API_KEY,

baseURL: process.env.DEEPSEEK_BASE_URL

});

```

```
// 只在服务端可用（API Routes、getServerSideProps 等）
process.env.DEEPSEEK_API_KEY  // 服务端可用
```

```
// 需要 NEXT_PUBLIC_ 前缀才能在客户端使用
process.env.NEXT_PUBLIC_API_BASE_URL  // 客户端和服务端都可用


```

```javascript

// 使用 js-tiktoken 计算 token 数量

import { encoding_for_model } from 'js-tiktoken';



function countTokens(text, model = 'gpt-4') {

const encoding = encoding_for_model(model);

const tokens = encoding.encode(text);

encoding.free(); // 释放内存

return tokens.length;

}



const text = "你好，世界！";

const tokenCount = countTokens(text);

console.log(`Token 数量：${tokenCount}`); // 输出：6

```


```javascript

// lib/llm-client.js

import OpenAI from 'openai';

import pRetry from 'p-retry';

import { encoding_for_model } from 'js-tiktoken';



export class LLMClient {

constructor(config) {

this.model = config.model || 'deepseek-chat';

this.maxTokens = config.maxTokens || 4096;



this.clients = config.apiKeys.map(key => new OpenAI({

apiKey: key,

baseURL: config.baseURL,

timeout: config.timeout || 60000,

maxRetries: config.maxRetries || 2

}));



this.currentIndex = 0;

}



// Token 计数

countTokens(messages) {

const encoding = encoding_for_model(this.model);

let total = 0;



for (const msg of messages) {

total += 4; // 每条消息固定开销

total += encoding.encode(msg.role).length;

total += encoding.encode(msg.content).length;

}



total += 2; // 对话结束标记

encoding.free();

return total;

}



// 获取下一个客户端

getNextClient() {

const client = this.clients[this.currentIndex];

this.currentIndex = (this.currentIndex + 1) % this.clients.length;

return client;

}



// 非流式对话

async chat(messages) {

return pRetry(

async () => {

const client = this.getNextClient();

const response = await client.chat.completions.create({

model: this.model,

messages

});

return response.choices[0].message.content;

},

{

retries: 3,

factor: 2,

minTimeout: 1000,

maxTimeout: 10000,

onFailedAttempt: (error) => {

console.log(`尝试 ${error.attemptNumber} 失败，还剩 ${error.retriesLeft} 次重试`);

}

}

);

}



// 流式对话

async *chatStream(messages) {

const client = this.getNextClient();



try {

const stream = await client.chat.completions.create({

model: this.model,

messages,

stream: true

});



for await (const chunk of stream) {

const content = chunk.choices[0]?.delta?.content || '';

if (content) {

yield content;

}

}

} catch (error) {

console.error('流式响应错误：', error);

throw error;

}

}

}



// 使用示例

const client = new LLMClient({

apiKeys: [

process.env.DEEPSEEK_API_KEY_1,

process.env.DEEPSEEK_API_KEY_2

],

baseURL: "https://api.deepseek.com",

model: "deepseek-chat",

timeout: 60000,

maxRetries: 3,

maxTokens: 4096

});



// 非流式调用

const response = await client.chat([

{ role: "system", content: "你是一个有用的助手。" },

{ role: "user", content: "什么是 TypeScript？" }

]);

console.log(response);



// 流式调用

for await (const chunk of client.chatStream([

{ role: "user", content: "讲一个故事。" }

])) {

process.stdout.write(chunk);

}

```


```javascript

async function callWithRateLimitHandling(prompt) {

    try {

        const response = await client.chat.completions.create({

            model: "deepseek-chat",

            messages: [{ role: "user", content: prompt }]

        });

        return response;

    } catch (error) {

        if (error.status === 429) {

            // 从响应头获取重试时间

            const retryAfter = error.headers?.['retry-after'];

            const waitTime = retryAfter ? parseInt(retryAfter) * 1000 : 5000;



            console.log(`触发限流，${waitTime}ms 后重试`);

            await new Promise(resolve => setTimeout(resolve, waitTime));



            // 递归重试

            return callWithRateLimitHandling(prompt);

        }

        throw error;

    }

}

```


```
/**
 * OpenAI 客户端类 - 支持多 API Key 轮询和自动重试
 * 用于处理 API 限流（429 错误）和多个 API Key 的负载均衡
 */
class OpenAIClient {

    /**
     * 构造函数 - 初始化多个 OpenAI 客户端实例
     * @param {string[]} apiKeys - API Key 数组，支持多个 Key 进行轮询
     */
    constructor(apiKeys) {
        // 为每个 API Key 创建一个 OpenAI 客户端实例
        this.clients = apiKeys.map(key => new OpenAI({
            apiKey: key,                                    // API Key
            baseURL: "https://api.deepseek.com",           // DeepSeek API 地址
            timeout: 60000                                  // 请求超时时间：60秒
        }));

        // 当前使用的客户端索引，用于轮询
        this.currentIndex = 0;
    }

    /**
     * 获取下一个客户端（轮询算法）
     * 使用轮询方式依次使用不同的 API Key，实现负载均衡
     * @returns {OpenAI} 返回下一个可用的 OpenAI 客户端实例
     */
    getNextClient() {
        // 获取当前索引对应的客户端
        const client = this.clients[this.currentIndex];
        
        // 更新索引，使用取模运算实现循环轮询
        // 例如：3个 Key，索引 0->1->2->0->1->2...
        this.currentIndex = (this.currentIndex + 1) % this.clients.length;

        return client;
    }

    /**
     * 发送聊天请求，支持自动重试和多个 API Key 切换
     * @param {string} prompt - 用户输入的提示词
     * @param {number} maxRetries - 最大重试次数，默认 3 次
     * @returns {Promise<string>} 返回 AI 生成的回复内容
     * @throws {Error} 如果所有 API Key 都失败，抛出错误
     */
    async chat(prompt, maxRetries = 3) {
        let lastError;  // 记录最后一次的错误信息

        // 循环重试，最多尝试 maxRetries 次
        for (let i = 0; i < maxRetries; i++) {
            // 获取下一个客户端（轮询切换 API Key）
            const client = this.getNextClient();

            try {
                // 调用 OpenAI API 发送聊天请求
                const response = await client.chat.completions.create({
                    model: "deepseek-chat",                          // 使用的模型
                    messages: [{ role: "user", content: prompt }]   // 消息内容
                });

                // 成功时返回 AI 的回复内容
                return response.choices[0].message.content;

            } catch (error) {
                // 记录错误信息
                lastError = error;
                console.log(`API Key ${this.currentIndex} 失败，尝试下一个...`);

                // 如果是 429 限流错误，等待 2 秒后重试
                // 429 表示请求过于频繁，需要等待一段时间
                if (error.status === 429) {
                    await new Promise(resolve => setTimeout(resolve, 2000));
                }
                // 其他错误（如网络错误、API Key 无效等）会继续尝试下一个 API Key
            }
        }

        // 如果所有重试都失败，抛出错误
        throw new Error(`所有 API Key 都失败了：${lastError.message}`);
    }

}

/**
 * 使用示例
 * 从环境变量中读取多个 API Key，创建客户端实例
 */
const client = new OpenAIClient([
    process.env.DEEPSEEK_API_KEY_1,  // 第一个 API Key
    process.env.DEEPSEEK_API_KEY_2,   // 第二个 API Key
    process.env.DEEPSEEK_API_KEY_3    // 第三个 API Key
]);

// 发送聊天请求
const response = await client.chat("你好");
```


# 6

```
在首页新增一个 tab【流式AI】，实现一个 AI 聊天功能：

模型调用参考 pages/api/tasks/breakdown.ts ，模型参数配置：温度 0.8、超时 120s、MaxTokens 65535、Seed 注释掉（稍后我会测试开启）；

模型平台选择硅基流动，baseURL 为 https://api.siliconflow.cn/v1 ，模型使用 deepseek-ai/DeepSeek-V3.2-Exp；

封装接口使用 p-retry 进行重试、SSE 流式输出、前端实现逐字输出结果（且不重复）；

每次请求使用 js-tiktoken 计算 token 数和成本（DeepSeek-V3.2 价格：输入 2元/百万tokens、输出 3元/百万tokens）；

遇到 429 错误，根据响应头中的 Retry-After 字段获取对方返回的重试时间进行重试

```